{
    "action_repeat": 1,
    "action_scale": 0.5,
    "ctrl_dt": 0.02,
    "episode_length": 100,
    "healthy_angle_range": [
        0,
        2.1
    ],
    "history_len": 1,
    "ppo_config": {
        "action_repeat": 1,
        "batch_size": 512,
        "clipping_epsilon": 0.3,
        "discounting": 0.97,
        "entropy_cost": 0.001,
        "episode_length": 100,
        "learning_rate": 0.0003,
        "max_grad_norm": 1.0,
        "network_factory": {
            "policy_hidden_layer_sizes": [
                50,
                50,
                50
            ],
            "policy_obs_key": "state",
            "value_hidden_layer_sizes": [
                50,
                50,
                50
            ],
            "value_obs_key": "state"
        },
        "normalize_observations": true,
        "num_envs": 8192,
        "num_evals": 16,
        "num_minibatches": 32,
        "num_resets_per_eval": 1,
        "num_timesteps": 40000000,
        "num_updates_per_batch": 8,
        "reward_scaling": 0.1,
        "unroll_length": 10
    },
    "reward_config": {
        "angle_reward_weight": 1,
        "bonus_weight": 4,
        "ctrl_cost_weight": 1,
        "pose_thd": 0.35
    },
    "sim_dt": 0.002
}