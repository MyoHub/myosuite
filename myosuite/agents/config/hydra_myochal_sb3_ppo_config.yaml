default:
    - override hydra/output: local
    - override hydra/launcher: local

env               :   'myoChallengeBimanual-v0' 
algorithm         :   PPO
seed              :   123
n_env             :   32
n_eval_env        :   5

# PPO object
policy            : 'MlpPolicy'
learning_rate     : 0.0003 
batch_size        : 64
gamma             : 0.99

# PPO.learn function
total_timesteps   : 1500000
log_interval      : 10000

eval_freq : 1000000
restore_checkpoint_freq : 500000
save_freq : 10000000

policy_kwargs:
  net_arch:
    - pi: [256, 256]
      vf: [256, 256]

# Algorithm hyperparameters : if alg requires additional params, can be specified here (or defaults will be used)
alg_hyper_params  :   {'device': 'cpu'}

clip_range: 0.2
ent_coeff: 0.001
n_epochs: 10
n_steps: 2048

job_name          :   ppo_sb3_${env}

hydra:
    job:
        name: ${env}